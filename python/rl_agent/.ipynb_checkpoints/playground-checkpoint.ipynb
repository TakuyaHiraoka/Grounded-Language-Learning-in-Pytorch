{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vision_M(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "            Use the same hyperparameter settings denoted in the paper\n",
    "        '''\n",
    "        \n",
    "        super(Vision_M, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=8, stride=4 )\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2 )\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is input image with shape [3, 84, 84]\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Language_M(nn.Module):\n",
    "    def __init__(self, vocab_size=100, embed_dim=128, hidden_size=128):\n",
    "        '''\n",
    "            Use the same hyperparameter settings denoted in the paper\n",
    "        '''\n",
    "        \n",
    "        super(Language_M, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)  # 2 words in vocab, 5 dimensional embeddings\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers=1, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            Argument\n",
    "                x: natural language instructions encoded in word indices, has shape\n",
    "                    [batch_size, seq]\n",
    "        '''\n",
    "        embedded_input = self.embeddings(x)\n",
    "        out, hn = self.lstm(embedded_input)\n",
    "        h, c = hn\n",
    "        \n",
    "        return h\n",
    "\n",
    "    def LP(self, x):\n",
    "        \n",
    "        return F.linear(x, self.embeddings.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size torch.Size([1, 10])\n",
      "Output size torch.Size([1, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "def test_language_module():\n",
    "    test_instruction = Variable(torch.LongTensor(np.random.randint(1, 100, size=(1, 10))))\n",
    "    print('Input size', test_instruction.size())\n",
    "\n",
    "    lm = Language_M()\n",
    "    output = lm(test_instruction)\n",
    "    \n",
    "    print('Output size', output.size())\n",
    "    \n",
    "    return output\n",
    "\n",
    "l_out = test_language_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size torch.Size([1, 3, 84, 84])\n",
      "Output size torch.Size([1, 64, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "def test_vision_module():\n",
    "    test_input = Variable(torch.randn(1, 3, 84, 84))\n",
    "    print('Input size', test_input.size())\n",
    "\n",
    "    vm = Vision_M()\n",
    "\n",
    "    test_output = vm(test_input)\n",
    "    print('Output size', test_output.size())\n",
    "\n",
    "    return test_output\n",
    "    \n",
    "v_out = test_vision_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Mixing_M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mixing_M, self).__init__()\n",
    "\n",
    "    \n",
    "    def forward(self, visual_encoded, instruction_encoded):\n",
    "        '''\n",
    "            Argument:\n",
    "                visual_encoded: output of vision module, shape [batch_size, 64, 7, 7]\n",
    "                instruction_encoded: hidden state of language module, shape [batch_size, 1, 128]\n",
    "        '''\n",
    "        batch_size = visual_encoded.size()[0]\n",
    "        visual_flatten = visual_encoded.view(batch_size, -1)\n",
    "        instruction_flatten = instruction_encoded.view(batch_size, -1)\n",
    "                \n",
    "        mixed = torch.cat([visual_flatten, instruction_flatten], dim=1)\n",
    "        \n",
    "        return mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size torch.Size([1, 3264])\n"
     ]
    }
   ],
   "source": [
    "def test_mixing_module(v_out, l_out):\n",
    "    mm = Mixing_M()\n",
    "    \n",
    "    test_output = mm(v_out, l_out)\n",
    "    print(\"Output size\", test_output.size())\n",
    "    \n",
    "    return test_output\n",
    "\n",
    "m_out = test_mixing_module(v_out, l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Action_M(nn.Module):\n",
    "    def __init__(self, batch_size=1, hidden_size=256):\n",
    "        super(Action_M, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm_1 = nn.LSTMCell(input_size=3264, hidden_size=256)\n",
    "        self.lstm_2 = nn.LSTMCell(input_size=256, hidden_size=256)\n",
    "        \n",
    "        self.hidden_1 = (Variable(torch.randn(batch_size, hidden_size)), \n",
    "                        Variable(torch.randn(batch_size, hidden_size))) \n",
    "        \n",
    "        self.hidden_2 = (Variable(torch.randn(batch_size, hidden_size)), \n",
    "                        Variable(torch.randn(batch_size, hidden_size))) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            Argument:\n",
    "                x: x is output from the Mixing Module, as shape [batch_size, 1, 3264]\n",
    "        '''\n",
    "        # Feed forward\n",
    "        h1, c1 = self.lstm_1(x, self.hidden_1)\n",
    "        h2, c2 = self.lstm_2(h1, self.hidden_2)\n",
    "        \n",
    "        # Update current hidden state\n",
    "        self.hidden_1 = (h1, c1)\n",
    "        self.hidden_2 = (h2, c2)\n",
    "        \n",
    "        # Return the hidden state of the upper layer\n",
    "        return h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "def test_action_module():\n",
    "    am = Action_M()\n",
    "    test_output = am(m_out)\n",
    "    print(test_output.size())\n",
    "    \n",
    "    return test_output\n",
    "\n",
    "am_output = test_action_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SavedAction = namedtuple('SavedAction', ['action', 'value'])\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(Policy, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.affine1 = nn.Linear(256, 128)\n",
    "        self.action_head = nn.Linear(128, action_space)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_scores = self.action_head(x)\n",
    "        state_values = self.value_head(x)\n",
    "        \n",
    "        return action_scores, state_values\n",
    "    \n",
    "    def tAE(self, action_logits):\n",
    "        '''\n",
    "        Temporal Autoencoder sub-task\n",
    "        Argument:\n",
    "            action_logits: shape [1, 10]\n",
    "        \n",
    "        Return:\n",
    "            output has shape: [1, 128]\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        output = action_logits - self.action_head.bias\n",
    "        output = F.linear(output, torch.transpose(self.action_head.weight, 0, 1))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size torch.Size([1, 10]) torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "def test_policy():\n",
    "    policy = Policy(action_space=10)\n",
    "    test_output = policy(am_output)\n",
    "    \n",
    "    print('Output size', test_output[0].size(), test_output[1].size())\n",
    "\n",
    "    return test_output\n",
    "\n",
    "policy_output = test_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # Core modules\n",
    "        self.vision_m = Vision_M()\n",
    "        self.language_m = Language_M()\n",
    "        self.mixing_m = Mixing_M()\n",
    "        self.action_m = Action_M()\n",
    "        \n",
    "        # Action selection and Value Critic\n",
    "        self.policy = Policy(action_space=10)\n",
    "        \n",
    "        # Auxiliary networks\n",
    "        self.tAE = temporal_AutoEncoder(self.policy)\n",
    "        \n",
    "        \n",
    "    def forward(self, img, instruction):\n",
    "        '''\n",
    "        Argument:\n",
    "        \n",
    "            img: environment image, shape [batch_size, 3, 84, 84]\n",
    "            instruction: natural language instruction [batch_size, seq]\n",
    "        '''\n",
    "        \n",
    "        vision_out = self.vision_m(img)\n",
    "        language_out = self.language_m(instruction)\n",
    "        mix_out = self.mixing_m(vision_out, language_out)\n",
    "        action_out = self.action_m(mix_out)\n",
    "        \n",
    "        action_prob, value = self.policy(action_out)\n",
    "        \n",
    "        return action_prob, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10]) torch.Size([1, 1])\n",
      "[[ 0.0226802   0.12301679  0.02969289  0.09865661 -0.05949788  0.03108864\n",
      "   0.04019292  0.01323096 -0.09392118  0.02453813]] [[ 0.10733929]]\n"
     ]
    }
   ],
   "source": [
    "def test_model():\n",
    "    model = Model()\n",
    "    \n",
    "    vision = Variable(torch.randn(1, 3, 84, 84))\n",
    "    instruction = Variable(torch.LongTensor(np.random.randint(1, 100, size=(1, 10))))\n",
    "    \n",
    "    probs, value = model(vision, instruction)\n",
    "    \n",
    "    print(probs.size(), value.size())\n",
    "    print(probs.data.numpy(), value.data.numpy())\n",
    "    \n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FakeEnvironment(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.generate_state()\n",
    "    \n",
    "    def step(self, action):\n",
    "        return self.generate_state(), 2, False, 1\n",
    "        \n",
    "    def generate_state(self):\n",
    "        vision = Variable(torch.randn(1, 3, 84, 84))\n",
    "        instruction = Variable(torch.LongTensor(np.random.randint(1, 100, size=(1, 10))))\n",
    "        \n",
    "        return (vision, instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "gamma = 0.99\n",
    "tau = 1.0\n",
    "model = Model()\n",
    "#model = ActorCritic(env.observation_space.shape[0], env.action_space)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "model.train()\n",
    "\n",
    "env = FakeEnvironment()\n",
    "state = env.reset()\n",
    "done = True\n",
    "\n",
    "episode_length = 0\n",
    "while True:\n",
    "    episode_length += 1\n",
    "\n",
    "    values = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    entropies = []\n",
    "\n",
    "    # Loop for number of unrolling steps\n",
    "    for step in range(50):\n",
    "        logit, value = model(*state)\n",
    "        \n",
    "        # Calculate entropy from action probability distribution\n",
    "        prob = F.softmax(logit)\n",
    "        log_prob = F.log_softmax(logit)\n",
    "        entropy = -(log_prob * prob).sum(1)\n",
    "        entropies.append(entropy)\n",
    "\n",
    "        # Take the action from distribution\n",
    "        action = prob.multinomial().data\n",
    "        log_prob = log_prob.gather(1, Variable(action))\n",
    "\n",
    "        # Perform the action on the environment\n",
    "        state, reward, done, _ = env.step(action.numpy())\n",
    "        done = done or episode_length >= 100\n",
    "        \n",
    "        if done:\n",
    "            episode_length = 0\n",
    "            state = env.reset()\n",
    "\n",
    "        values.append(value)\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Terminal state or reached number of unrolling steps, whichever comes first\n",
    "    R, gae = Variable(torch.zeros(1, 1)), torch.zeros(1, 1)\n",
    "    \n",
    "    # Get value estimate of current state\n",
    "    if not done:\n",
    "        _, R = model(*state)\n",
    "\n",
    "    values.append(R)\n",
    "    policy_loss, value_loss = 0, 0\n",
    "\n",
    "    # Performing update\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        # Value function loss\n",
    "        R = gamma * R + rewards[i]\n",
    "        value_loss = value_loss + 0.5 * (R - values[i]).pow(2)\n",
    "\n",
    "        # Generalized Advantage Estimataion\n",
    "        delta_t = rewards[i] + gamma * \\\n",
    "                values[i + 1].data - values[i].data\n",
    "        gae = gae * gamma * tau + delta_t\n",
    "\n",
    "        # Computing policy loss\n",
    "        policy_loss = policy_loss - \\\n",
    "            log_probs[i] * Variable(gae) - 0.01 * entropies[i]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Back-propagation\n",
    "    (policy_loss + 0.5 * value_loss).backward()\n",
    "    torch.nn.utils.clip_grad_norm(model.parameters(), 40)\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    # This is only for testing\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-53e57142b595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mtemporal_AutoEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvision_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemporal_AutoEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvision_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class temporal_AutoEncoder(nn.Module):\n",
    "    def __init__(self, policy_network, vision_module):\n",
    "        super(temporal_AutoEncoder, self).__init__()\n",
    "        self.policy_network = policy_network\n",
    "        self.vision_module = vision_module\n",
    "    \n",
    "        self.linear_1 = nn.Linear(128, 64 * 7 * 7)\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "                        nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=4, stride=2),\n",
    "                        nn.ConvTranspose2d(in_channels=32, out_channels=3,  kernel_size=8, stride=4))\n",
    "    \n",
    "    def forward(self, visual_input, logit_action):\n",
    "        '''\n",
    "        Argument:\n",
    "            visual_encoded: output from the visual module, has shape [1, 64, 7, 7]\n",
    "            logit_action: output action logit from policy, has shape [1, 10]\n",
    "        '''\n",
    "        visual_encoded = self.vision_module(visual_input)\n",
    "        \n",
    "        action_out = self.policy_network.tAE(logit_action) # [1, 128]\n",
    "        action_out = self.linear_1(action_out)\n",
    "        action_out = action_out.view(action_out.size()[0], 64, 7, 7)\n",
    "        \n",
    "        combine = torch.mul(action_out, visual_encoded)\n",
    "        \n",
    "        out = self.deconv(combine)\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size tAE torch.Size([1, 3, 84, 84])\n"
     ]
    }
   ],
   "source": [
    "def test_tAE():\n",
    "    v_input = Variable(torch.randn(1, 3, 84, 84))\n",
    "    \n",
    "    policy = Policy(action_space=10)\n",
    "    vm = Vision_M()\n",
    "    tAE = temporal_AutoEncoder(policy, vm)\n",
    "    \n",
    "    test_output = policy(am_output)\n",
    "    test_tae = tAE(v_input, test_output[0])\n",
    "    print('Output size tAE', test_tae.size())\n",
    "\n",
    "    return test_output\n",
    "\n",
    "policy_output = test_tAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Language_Prediction(nn.Module):\n",
    "    def __init__(self, language_module):\n",
    "        super(Language_Prediction, self).__init__()\n",
    "        self.language_module = language_module\n",
    "        \n",
    "        self.vision_transform = nn.Sequential(\n",
    "                            nn.Linear(64 * 7 * 7, 128),\n",
    "                            nn.ReLU())\n",
    "    \n",
    "    def forward(self, vision_encoded):\n",
    "            \n",
    "        vision_encoded_flatten = vision_encoded.view(vision_encoded.size()[0], -1)\n",
    "        vision_out = self.vision_transform(vision_encoded_flatten)\n",
    "        \n",
    "        language_predict = self.language_module.LP(vision_out)\n",
    "        \n",
    "        return language_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size torch.Size([1, 1, 128])\n",
      "torch.Size([1, 100])\n"
     ]
    }
   ],
   "source": [
    "def test_language_prediction():\n",
    "    test_instruction = Variable(torch.LongTensor(np.random.randint(1, 100, size=(1, 10))))\n",
    "    lm = Language_M()\n",
    "    output = lm(test_instruction)\n",
    "    \n",
    "    print('Output size', output.size())\n",
    "\n",
    "    lp = Language_Prediction(lm)\n",
    "    lp_out = lp(v_out)\n",
    "    \n",
    "    print(lp_out.size())\n",
    "    \n",
    "    return output\n",
    "\n",
    "l_out = test_language_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RewardPrediction(nn.Module):\n",
    "    def __init__(self, vision_module):\n",
    "        super(RewardPrediction, self).__init__()\n",
    "        \n",
    "        self.vision_module = vision_module\n",
    "        self.linear = nn.Linear(3 * 64 * 7 * 7, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            x: state including image and instruction, each batch contains 3 image in sequence \n",
    "                    with the instruction to be encoded\n",
    "        '''\n",
    "        batch_visual = []\n",
    "        batch_instruction = []\n",
    "        \n",
    "        for batch in x:\n",
    "            visual = [b.visual for b in batch]\n",
    "            instruction = [b.instruction for b in batch]\n",
    "            \n",
    "            batch_visual.append(visual)\n",
    "            batch_instruction.append(instruction)\n",
    "        \n",
    "        # Will Need some reshaping here\n",
    "        batch_visual_encoded = self.vision_module(batch_visual)\n",
    "        batch_instruction_encoded = self.language_module(batch_instruction)\n",
    "        \n",
    "        batch_mixed = self.mixed_module(batch_visual_encoded, batch_instruction_encoded)\n",
    "        \n",
    "        # Might need some stuff here as well.\n",
    "        total = torch.cat([out_1, out_2, out_3], dim=1)\n",
    "        out = self.linear(total)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "def test_reward_predictor():\n",
    "    test_1 = Variable(torch.randn(1, 3, 84, 84))\n",
    "    test_2 = Variable(torch.randn(1, 3, 84, 84))\n",
    "    test_3 = Variable(torch.randn(1, 3, 84, 84))\n",
    "\n",
    "    vm = Vision_M()\n",
    "\n",
    "    rp = RewardPrediction(vm)\n",
    "    out = rp(test_1, test_2, test_3)\n",
    "    print('Output size', out.size())\n",
    "    \n",
    "    return out\n",
    "    \n",
    "rp_out = test_reward_predictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
